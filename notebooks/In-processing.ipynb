{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook demonstrates how to Identify & Remove Bias during the \"In-Processing Stage\" using the AI Fairness 360 Toolkit by adjusting the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-processing algorithm\n",
    "A bias mitigation algorithm that is applied to a model during its training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The following code contains the credentials for a file in your IBM Cloud Object Storage.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "credentials = {\n",
    "    'IAM_SERVICE_ID': 'IAM_SERVICE_ID',\n",
    "    'IBM_API_KEY_ID': 'IBM_API_KEY_ID',\n",
    "    'ENDPOINT': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'IBM_AUTH_ENDPOINT': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'BUCKET': 'ai360series-donotdelete-pr-1zay1c1uizeatc',\n",
    "    'FILE': 'Pipeline_LabelEncoder-0.1.zip'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "cos = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=credentials['IBM_API_KEY_ID'],\n",
    "    ibm_service_instance_id=credentials['IAM_SERVICE_ID'],\n",
    "    ibm_auth_endpoint=credentials['IBM_AUTH_ENDPOINT'],\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=credentials['ENDPOINT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos.download_file(Bucket=credentials['BUCKET'],Key='Pipeline_LabelEncoder-0.1.zip',Filename='/home/wsuser/work/Pipeline_LabelEncoder-0.1.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install Pipeline_LabelEncoder-0.1.zip\n",
    "!pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install 'tensorflow>=1.13.1,< 2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load all necessary packages\n",
    "import pandas as pd\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "from aif360.metrics.utils import compute_boolean_conditioning_vector\n",
    "\n",
    "from aif360.algorithms.inprocessing.adversarial_debiasing import AdversarialDebiasing\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where you import your DataFrame. Delete everything in this cell below this comment. \n",
    "# Import your data with \"Find and add data\" in the top right 0100 icon. \n",
    "# Select \"Insert to code => pandas DataFrame\" under the data you want to import.\n",
    "\n",
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_46fb316bd6aa42a880152b79a0bc6af1 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='IBM_API_KEY_ID',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body = client_46fb316bd6aa42a880152b79a0bc6af1.get_object(Bucket='aifairness360-donotdelete-pr-jf2oqgkybwubjd',Key='fraud_data.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "df_data_1 = pd.read_csv(body)\n",
    "df_data_1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_1.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'Gender': 1}]\n",
    "unprivileged_groups = [{'Gender': 0}]\n",
    "favorable_label = 1 \n",
    "unfavorable_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "categorical_column = ['Gender', 'Married', 'Fraud_risk']\n",
    "\n",
    "data_encoded = df_data_1.copy(deep=True)\n",
    "#Use Scikit-learn label encoding to encode character data\n",
    "lab_enc = preprocessing.LabelEncoder()\n",
    "for col in categorical_column:\n",
    "        data_encoded[col] = lab_enc.fit_transform(df_data_1[col])\n",
    "        le_name_mapping = dict(zip(lab_enc.classes_, lab_enc.transform(lab_enc.classes_)))\n",
    "        print('Feature', col)\n",
    "        print('mapping', le_name_mapping)\n",
    "        \n",
    "\n",
    "data_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Pipeline_LabelEncoder.sklearn_label_encoder import PipelineLabelEncoder\n",
    "preprocessed_data = PipelineLabelEncoder(columns = ['Gender','Married', 'Fraud_risk']).fit_transform(data_encoded)\n",
    "print('-------------------------')\n",
    "#print('validation data encoding')\n",
    "#validation_enc_data = PipelineLabelEncoder(columns = ['Gender','Married', 'Fraud_risk']).transform(validation_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create binary label dataset that can be used by bias mitigation algorithms\n",
    "fraud_dataset = BinaryLabelDataset(favorable_label=favorable_label,\n",
    "                                unfavorable_label=unfavorable_label,\n",
    "                                df=preprocessed_data,\n",
    "                                label_names=['Fraud_risk'],\n",
    "                                protected_attribute_names=['Gender', 'Married'],\n",
    "                                unprivileged_protected_attributes=unprivileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Training Data Details\"))\n",
    "print(\"shape of the training dataset\", fraud_dataset.features.shape)\n",
    "print(\"Training data favorable label\", fraud_dataset.favorable_label)\n",
    "print(\"Training data unfavorable label\", fraud_dataset.unfavorable_label)\n",
    "print(\"Training data protected attribute\", fraud_dataset.protected_attribute_names)\n",
    "print(\"Training data privileged protected attribute (1:Male and 0:Female)\", \n",
    "      fraud_dataset.privileged_protected_attributes)\n",
    "print(\"Training data unprivileged protected attribute (1:Male and 0:Female)\",\n",
    "      fraud_dataset.unprivileged_protected_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_dataset_train, fraud_dataset_test = fraud_dataset.split([0.9], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric for the original dataset\n",
    "metric_orig_train = BinaryLabelDatasetMetric(fraud_dataset_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Original training dataset\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())\n",
    "metric_orig_test = BinaryLabelDatasetMetric(fraud_dataset_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_test.mean_difference())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_scaler = MaxAbsScaler()\n",
    "fraud_dataset_train.features = min_max_scaler.fit_transform(fraud_dataset_train.features)\n",
    "fraud_dataset_test.features = min_max_scaler.transform(fraud_dataset_test.features)\n",
    "metric_scaled_train = BinaryLabelDatasetMetric(fraud_dataset_train, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "display(Markdown(\"#### Scaled dataset - Verify that the scaling does not affect the group label statistics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_train.mean_difference())\n",
    "metric_scaled_test = BinaryLabelDatasetMetric(fraud_dataset_test, \n",
    "                             unprivileged_groups=unprivileged_groups,\n",
    "                             privileged_groups=privileged_groups)\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_scaled_test.mean_difference())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build plan classifier without debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load post-processing algorithm that equalizes the odds\n",
    "# Learn parameters with debias set to False\n",
    "sess = tf.Session()\n",
    "#sess = tf.compat.v1.Session()\n",
    "plain_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='plain_classifier',\n",
    "                          debias=False,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_model.fit(fraud_dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the plain model to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_nodebiasing_train = plain_model.predict(fraud_dataset_train)\n",
    "dataset_nodebiasing_test = plain_model.predict(fraud_dataset_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for the dataset from plain model (without debiasing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"#### Model - without debiasing - dataset metrics\"))\n",
    "metric_dataset_nodebiasing_train = BinaryLabelDatasetMetric(dataset_nodebiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_nodebiasing_test = BinaryLabelDatasetMetric(dataset_nodebiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "display(Markdown(\"#### Model - without debiasing - classification metrics\"))\n",
    "classified_metric_nodebiasing_test = ClassificationMetric(fraud_dataset_test, \n",
    "                                                 dataset_nodebiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply in-processing algorithm based on adversarial learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn parameters with debias set to True\n",
    "debiased_model = AdversarialDebiasing(privileged_groups = privileged_groups,\n",
    "                          unprivileged_groups = unprivileged_groups,\n",
    "                          scope_name='debiased_classifier',\n",
    "                          debias=True,\n",
    "                          sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debiased_model.fit(fraud_dataset_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the plain model to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_debiasing_train = debiased_model.predict(fraud_dataset_train)\n",
    "dataset_debiasing_test = debiased_model.predict(fraud_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics for the dataset from plain model (without debiasing)\n",
    "display(Markdown(\"#### Model - without debiasing - dataset metrics\"))\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_train.mean_difference())\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_nodebiasing_test.mean_difference())\n",
    "\n",
    "# Metrics for the dataset from model with debiasing\n",
    "display(Markdown(\"#### Model - with debiasing - dataset metrics\"))\n",
    "metric_dataset_debiasing_train = BinaryLabelDatasetMetric(dataset_debiasing_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Train set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_train.mean_difference())\n",
    "\n",
    "metric_dataset_debiasing_test = BinaryLabelDatasetMetric(dataset_debiasing_test, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Test set: Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_dataset_debiasing_test.mean_difference())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - without debiasing - classification metrics\"))\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_nodebiasing_test.accuracy())\n",
    "TPR = classified_metric_nodebiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_nodebiasing_test.true_negative_rate()\n",
    "bal_acc_nodebiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_nodebiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_nodebiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_nodebiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_nodebiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_nodebiasing_test.theil_index())\n",
    "\n",
    "\n",
    "\n",
    "display(Markdown(\"#### Model - with debiasing - classification metrics\"))\n",
    "classified_metric_debiasing_test = ClassificationMetric(fraud_dataset_test, \n",
    "                                                 dataset_debiasing_test,\n",
    "                                                 unprivileged_groups=unprivileged_groups,\n",
    "                                                 privileged_groups=privileged_groups)\n",
    "print(\"Test set: Classification accuracy = %f\" % classified_metric_debiasing_test.accuracy())\n",
    "TPR = classified_metric_debiasing_test.true_positive_rate()\n",
    "TNR = classified_metric_debiasing_test.true_negative_rate()\n",
    "bal_acc_debiasing_test = 0.5*(TPR+TNR)\n",
    "print(\"Test set: Balanced classification accuracy = %f\" % bal_acc_debiasing_test)\n",
    "print(\"Test set: Disparate impact = %f\" % classified_metric_debiasing_test.disparate_impact())\n",
    "print(\"Test set: Equal opportunity difference = %f\" % classified_metric_debiasing_test.equal_opportunity_difference())\n",
    "print(\"Test set: Average odds difference = %f\" % classified_metric_debiasing_test.average_odds_difference())\n",
    "print(\"Test set: Theil_index = %f\" % classified_metric_debiasing_test.theil_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We have observed how to use the AI Fairness 360 Toolkit to eliminate bias during the Pre-Processing & In-Processing Stages of model building & development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
